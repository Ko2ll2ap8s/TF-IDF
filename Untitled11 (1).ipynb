{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa202c2a",
   "metadata": {},
   "source": [
    "#### TF-IDF - Если слово часто встречается во всех документах (это в первую очередь касается предлогов, союзов и других стоп-слов), то вряд ли эти слова имеют большое значение. И наоборот, если слово встречаться только в одном документе, вероятно оно в большей степени определяет его содержание.\n",
    "\n",
    "#### Word2vec - Программа для анализа текста. Используется для анализа семантики естественных языков, основанный на дистрибутивной семантике, машинном обучении и векторном представлении слов.\n",
    "\n",
    "#### Bert - это метод машинного обучения на основе трансформера для предварительной подготовки к обработке естественного языка (NLP), разработанный Google. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0605d164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'we', 'were', 'in', 'Paris', 'we', 'visited', 'a', 'lot', 'of', 'museums', '.', 'We', 'first', 'went', 'to', 'the', 'Louvre', ',', 'the', 'largest', 'art', 'museum', 'in', 'the', 'world', '.', 'I', 'have', 'always', 'been', 'interested', 'in', 'art', 'so', 'I', 'spent', 'many', 'hours', 'there', '.', 'The', 'museum', 'is', 'enourmous', ',', 'so', 'a', 'week', 'there', 'would', 'not', 'be', 'enough', '.']\n"
     ]
    }
   ],
   "source": [
    "# импортируем метод word_tokenize - Токенизаторы делят строки на списки подстроек\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = []\n",
    "# в цикле for пройдемся по каждому предложению\n",
    "for sentence in sentences:\n",
    " \n",
    "    # создадим списки из токенов\n",
    "    t = word_tokenize(sentence)\n",
    " \n",
    "    # и присоединим списки друг к другу\n",
    "    tokens.extend(t)\n",
    " \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8ca685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['When we were in Paris we visited a lot of museums.',\n",
       " 'We first went to the Louvre, the largest art museum in the world.',\n",
       " 'I have always been interested in art so I spent many hours there.',\n",
       " 'The museum is enourmous, so a week there would not be enough.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# исходный текст для анализа\n",
    "corpus = 'When we were in Paris we visited a lot of museums. We first went to the Louvre, the largest art museum in the world. I have always been interested in art so I spent many hours there. The museum is enourmous, so a week there would not be enough.'\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# импортируем метод sent_tokenize - используется для разделения строки или абзаца на предложения\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# модель, которая будет делить на предложения\n",
    "nltk.download('punkt')\n",
    "# применяем метод к тексту\n",
    "sentences = sent_tokenize(corpus)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bafb8d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'visited', 'lot', 'museums', 'first', 'went', 'louvre', 'largest', 'art', 'museum', 'world', 'always', 'interested', 'art', 'spent', 'many', 'hours', 'museum', 'enourmous', 'week', 'would', 'enough']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# импортируем модуль стоп-слов\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# скачаем словарь стоп-слов\n",
    "nltk.download('stopwords')\n",
    " \n",
    "# используем set, чтобы оставить только уникальные значения\n",
    "unique_stops = set(stopwords.words('english'))\n",
    " \n",
    "# создаём пустой список без стоп-слов\n",
    "no_stops = []\n",
    " \n",
    "# проходимся по всем токенам\n",
    "for token in tokens:\n",
    " \n",
    "    # переводим все слова в нижний регистр\n",
    "    token = token.lower()\n",
    " \n",
    "    # если токен не в списке стоп-слов и не является знаком пунктуации\n",
    "    if token not in unique_stops and token.isalpha():\n",
    " \n",
    "        # добавляем его в список\n",
    "        no_stops.append(token)\n",
    " \n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d983330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'visited', 'lot', 'museum', 'first', 'went', 'louvre', 'largest', 'art', 'museum', 'world', 'always', 'interested', 'art', 'spent', 'many', 'hour', 'museum', 'enourmous', 'week', 'would', 'enough']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# импортируем класс для лемматизации\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# импортируем словарь\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# создаём объект этого класса\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "# и пустой список для слов после лемматизации\n",
    "lemmatized = []\n",
    " \n",
    "# проходимся по всем токенам\n",
    "for token in no_stops:\n",
    " \n",
    "    # применяем лемматизацию\n",
    "    token = lemmatizer.lemmatize(token)\n",
    " \n",
    "    # добавляем слово после лемматизации в список\n",
    "    lemmatized.append(token)\n",
    "    \n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a52f32f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pari', 'visit', 'lot', 'museum', 'first', 'went', 'louvr', 'largest', 'art', 'museum', 'world', 'alway', 'interest', 'art', 'spent', 'mani', 'hour', 'museum', 'enourm', 'week', 'would', 'enough']\n"
     ]
    }
   ],
   "source": [
    "# импортируем класс стеммера Porter и создаём объект этого класса\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    " \n",
    "# используем list comprehension вместо цикла for для стемминга и создания нового списка\n",
    "# такая запись намного короче\n",
    "stemmed_p = [porter.stem(s) for s in lemmatized]\n",
    "print(stemmed_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b4138b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['par', 'visit', 'lot', 'muse', 'first', 'went', 'louvr', 'largest', 'art', 'muse', 'world', 'alway', 'interest', 'art', 'spent', 'many', 'hour', 'muse', 'enourm', 'week', 'would', 'enough']\n"
     ]
    }
   ],
   "source": [
    "# аналогично импортируем класс Lancaster и создаём объект этого класса\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    " \n",
    "# также используем list_comprehension\n",
    "stemmed_l = [lancaster.stem(s) for s in lemmatized]\n",
    "print(stemmed_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d67567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('museum', 3), ('art', 2), ('paris', 1), ('visited', 1), ('lot', 1), ('first', 1), ('went', 1), ('louvre', 1), ('largest', 1), ('world', 1)]\n"
     ]
    }
   ],
   "source": [
    "# из модуля collections импортируем класс Counter\n",
    "from collections import Counter\n",
    " \n",
    "# применяем класс Counter к словам после лемматизации\n",
    "# на выходе нам возвращается словарь { слово : его частота в тексте }\n",
    "bow_counter = Counter(lemmatized)\n",
    "# print(bow_counter)\n",
    " \n",
    "# функция most_common() упорядочивает словарь по значению\n",
    "# посмотрим на первые 10 наиболее частотных слов\n",
    "print(bow_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "011cc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем класс CountVectorizer из библиотеки Scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "# создаём объект этого класса и\n",
    "# указываем, что хотим перевести слова в нижний регистр, а также\n",
    "# отфильтровать стоп-слова через stop_words = {'english'}\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             lowercase = True, \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = {'english'}, \n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d19eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# применяем этот объект к предложениям \n",
    "bow_cv = vectorizer.fit_transform(sentences)\n",
    " \n",
    "# на выходе получается матрица csr\n",
    "print(type(bow_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4386cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 2 0 0 1 1 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 3 0 1 0 1 0 1 0 0 1 0]\n",
      " [1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# для этого можно использовать .toarray()\n",
    "print(bow_cv.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a3fdf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 34)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0815fec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['always', 'art', 'be', 'been', 'enough', 'enourmous', 'first',\n",
       "       'have', 'hours', 'in', 'interested', 'is', 'largest', 'lot',\n",
       "       'louvre', 'many', 'museum', 'museums', 'not', 'of', 'paris', 'so',\n",
       "       'spent', 'the', 'there', 'to', 'visited', 'we', 'week', 'went',\n",
       "       'were', 'when', 'world', 'would'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = vectorizer.get_feature_names_out()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b0a7b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>always</th>\n",
       "      <th>art</th>\n",
       "      <th>be</th>\n",
       "      <th>been</th>\n",
       "      <th>enough</th>\n",
       "      <th>enourmous</th>\n",
       "      <th>first</th>\n",
       "      <th>have</th>\n",
       "      <th>hours</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>there</th>\n",
       "      <th>to</th>\n",
       "      <th>visited</th>\n",
       "      <th>we</th>\n",
       "      <th>week</th>\n",
       "      <th>went</th>\n",
       "      <th>were</th>\n",
       "      <th>when</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence_1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence_2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            always  art  be  been  enough  enourmous  first  have  hours  in  \\\n",
       "Sentence_0       0    0   0     0       0          0      0     0      0   1   \n",
       "Sentence_1       0    1   0     0       0          0      1     0      0   1   \n",
       "Sentence_2       1    1   0     1       0          0      0     1      1   1   \n",
       "Sentence_3       0    0   1     0       1          1      0     0      0   0   \n",
       "\n",
       "            ...  there  to  visited  we  week  went  were  when  world  would  \n",
       "Sentence_0  ...      0   0        1   2     0     0     1     1      0      0  \n",
       "Sentence_1  ...      0   1        0   1     0     1     0     0      1      0  \n",
       "Sentence_2  ...      1   0        0   0     0     0     0     0      0      0  \n",
       "Sentence_3  ...      1   0        0   0     1     0     0     0      0      1  \n",
       "\n",
       "[4 rows x 34 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вначале создадим индекс предложений\n",
    "index_list = []\n",
    " \n",
    "# в цикле пройдемся по элементам матрицы, обозначим их через '_'\n",
    "# функция enumerate задаст каждому элементу индекс, начиная с 0\n",
    "for i, _ in enumerate(bow_cv):\n",
    " \n",
    "    # прибавим наш индекс к слову Sentence \n",
    "    index_list.append(f'Sentence_{i}')\n",
    " \n",
    "bow_cv_df = pd.DataFrame(data = bow_cv.toarray(), \n",
    "                         index = index_list, \n",
    "                         columns = tokens)\n",
    "bow_cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c53bf203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x34 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72f4b672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x34 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импортируем TfidfTransformer (CountVectorizer уже импортирован)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "# создадим объект класса TfidfTransformer\n",
    "tfidf_trans = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    " \n",
    "# и рассчитаем IDF слов\n",
    "tfidf_trans.fit(bow_cv)\n",
    " \n",
    "# поместим результат в датафрейм\n",
    "df_idf = pd.DataFrame(tfidf_trans.idf_, index = tokens, columns = [\"idf_weights\"])\n",
    "\n",
    "# рассчитаем TF-IDF (по сути умножим TF на IDF)\n",
    "tf_idf_vector = tfidf_trans.transform(bow_cv)\n",
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "738c58d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0         1         2         3\n",
      "always      0.000000  0.000000  0.328404  0.000000\n",
      "art         0.000000  0.211724  0.258918  0.000000\n",
      "be          0.000000  0.000000  0.000000  0.324676\n",
      "been        0.000000  0.000000  0.328404  0.000000\n",
      "enough      0.000000  0.000000  0.000000  0.324676\n",
      "enourmous   0.000000  0.000000  0.000000  0.324676\n",
      "first       0.000000  0.268544  0.000000  0.000000\n",
      "have        0.000000  0.000000  0.328404  0.000000\n",
      "hours       0.000000  0.000000  0.328404  0.000000\n",
      "in          0.202925  0.171408  0.209616  0.000000\n",
      "interested  0.000000  0.000000  0.328404  0.000000\n",
      "is          0.000000  0.000000  0.000000  0.324676\n",
      "largest     0.000000  0.268544  0.000000  0.000000\n",
      "lot         0.317921  0.000000  0.000000  0.000000\n",
      "louvre      0.000000  0.268544  0.000000  0.000000\n",
      "many        0.000000  0.000000  0.328404  0.000000\n",
      "museum      0.000000  0.211724  0.000000  0.255978\n",
      "museums     0.317921  0.000000  0.000000  0.000000\n",
      "not         0.000000  0.000000  0.000000  0.324676\n",
      "of          0.317921  0.000000  0.000000  0.000000\n",
      "paris       0.317921  0.000000  0.000000  0.000000\n",
      "so          0.000000  0.000000  0.258918  0.255978\n",
      "spent       0.000000  0.000000  0.328404  0.000000\n",
      "the         0.000000  0.635171  0.000000  0.255978\n",
      "there       0.000000  0.000000  0.258918  0.255978\n",
      "to          0.000000  0.268544  0.000000  0.000000\n",
      "visited     0.317921  0.000000  0.000000  0.000000\n",
      "we          0.501305  0.211724  0.000000  0.000000\n",
      "week        0.000000  0.000000  0.000000  0.324676\n",
      "went        0.000000  0.268544  0.000000  0.000000\n",
      "were        0.317921  0.000000  0.000000  0.000000\n",
      "when        0.317921  0.000000  0.000000  0.000000\n",
      "world       0.000000  0.268544  0.000000  0.000000\n",
      "would       0.000000  0.000000  0.000000  0.324676\n"
     ]
    }
   ],
   "source": [
    "# для этого переведем матрицу csr в обычный массив Numpy\n",
    "df_tfidf = pd.DataFrame(tf_idf_vector.toarray(), columns = vectorizer.get_feature_names_out())\n",
    " \n",
    "# и траспонируем его (запишем столбцы в виде строк) \n",
    "print(df_tfidf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73a529a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a296c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
